# -*- coding: utf-8 -*-
"""Meditron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P9jCcKnYlF5BF1zFnKVJUu57o9XaCiqj
"""

!pip install pypdf
!pip install torch
!pip install sentence_transformers
!pip install transformers
!pip install langchain

!pip install PyPDF2

!pip install chroma
!pip install qdrant-client
!pip install ctransformers
!pip install langchain_community

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import Qdrant
from langchain.document_loaders import DirectoryLoader
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI

embeddings = SentenceTransformerEmbeddings(model_name="NeuML/pubmedbert-base-embeddings")

print(embeddings)

documents = DirectoryLoader('/content/sample_data', show_progress = True, loader_cls = PyPDFLoader).load()
print(documents[2])

text_splitter = RecursiveCharacterTextSplitter(chunk_size = 100, chunk_overlap = 1 )
texts = text_splitter.split_documents(documents)

!pip install chromadb

import chromadb
client = chromadb.Client()

from langchain_community.vectorstores import Chroma
db = Chroma.from_documents(texts, embeddings, client=client)

vectorstore = Chroma(embedding_function=embeddings)

# Create a retriever from the vector store
retriever = vectorstore.as_retriever()

print(db)
print("##########################")

query = "What are the common side effects of systematic therapuetic agents"
docs = db.similarity_search_with_score(query = query, k=1)

contexts = []
for i in docs:
  context = i # Assuming 'text' contains the document text
  contexts.append(context)
  doc, score = i
  print({"Score": score, "Content": doc.page_content, "metadata": doc.metadata})

#Based on the output you can implement long context reordring , by doing so
# Reorder the documents:
# Less relevant document will be at the middle of the list and more
# relevant elements at beginning / end.

from langchain_community.document_transformers import LongContextReorder

docs_temp = docs
reordering = LongContextReorder()
reordered_docs = reordering.transform_documents(docs_temp)

for i in reordered_docs:
 doc, score = i
 print({"Score": score, "Content": doc.page_content, "metadata": doc.metadata})

!!huggingface-cli login

!pip install CTransformers

from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM, AutoModel
import torch
from transformers import BitsAndBytesConfig
from langchain_community.llms import CTransformers

!pip install transformers accelerate

tokenizer = AutoTokenizer.from_pretrained("epfl-llm/meditron-7b")
model = AutoModelForCausalLM.from_pretrained("epfl-llm/meditron-7b")

model2 = "epfl-llm/meditron-7b"

!pip install llama-cpp-python

from huggingface_hub import hf_hub_download
model_basename = "llama-2-70b-chat.ggmlv3.q4_0.bin"

model_path = hf_hub_download(repo_id="TheBloke/meditron-7B-chat-GGUF", filename="meditron-7b-chat.Q4_K_M.gguf")

from langchain_community.llms import LlamaCpp

from llama_cpp import Llama

# llm = Llama(
#     model_path = "epfl-llm/meditron-7b",
#     model_type="llama",
#     tokenizer = tokenizer,
#     context_window=2048,
#     max_new_tokens=512,
#     generate_kwargs={"temperature": 0.0, "do_sample": False})

llm = LlamaCpp(
    model_path = model_path,
    context_window=2048,
    max_new_tokens=512,
    generate_kwargs={"temperature": 0.3, "do_sample": False}
 )

# llm = CTransformers(
#     model = model2,
#     model_type="llama2",
#     tokenizer = tokenizer,
#     context_window=2048,
#     max_new_tokens=512,
#     generate_kwargs={"temperature": 0.0, "do_sample": False})

question = """
Question: What is the cure to cancer?
"""
llm.invoke(question)

question = """
Question: What are the common side effects of systematic therapuetic agents?
"""
llm.invoke(question)

!pip install langchain

from langchain.chains import RetrievalQA

#chain_type_kwargs = {"prompt": prompt}
qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())
#response = qa({"question": "What are the common side effects of systematic therapuetic agents?"})
print(qa.run("What are the common side effects of systematic therapuetic agents?"))

from langchain.chains import ConversationalRetrievalChain

query = "What are the common side effects of systematic therapuetic agents?"

qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())

chat_history = []
result = qa({"query": query, "chat_history": chat_history})
print(result['result'])

chat_history = [(query, result['result'])]
query = "Will a side effect of this be skin rashes on hands or legs? "
result = qa({"query": query, "chat_history": chat_history})
print(result)



